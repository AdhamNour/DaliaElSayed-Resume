\item \textbf{Senior Data Engineer} \hfill Al-Rajhi Bank (ARB), Riyadh, Saudi Arabia (\textit{January 2025 â€“ Present})
\input{experience/ARB/OrganizasionIntroduction.tex}
\begin{itemize}
    \item \textbf{Modern Data Stack Proof of Concept (PoC) -In Progress-:}
    \begin{itemize}
        \item \textbf{Architecture Modernization:} Spearheading a strategic migration from legacy Informatica and Oracle DB silos to a decoupled \textbf{Open Data Lakehouse} architecture.
        \item \textbf{Infrastructure Evolution:} Architected a storage-compute decoupling strategy using \textbf{Apache Iceberg} and \textbf{OCI S3-Compatible Object Storage}, projected to significantly reduce licensing overhead.
        \item \textbf{Airflow-dbt Implementation:} Replaced rigid legacy mappings with \textbf{Apache Airflow} and \textbf{dbt} for 5+ critical workflows, enhancing version control and developer velocity.
        \item \textbf{High-Performance Analytics:} Integrated \textbf{Trino} as the primary query engine to facilitate low-latency interactive analytics over Iceberg tables for enterprise BI tools.
    \end{itemize}

    \item \textbf{SIMAH Reporting}
    \begin{itemize}
        \item \textbf{Regulatory Compliance:} Engineered end-to-end data pipelines to aggregate and transform credit risk data for submission to \textbf{SIMAH}, Saudi Arabia's leading credit bureau.
        \item \textbf{Performance Optimization:} Optimized ETL workflows to reduce data processing time by \textbf{30\%}, ensuring timely report generation ahead of regulatory deadlines.
    \end{itemize}
    \item \textbf{Data Stream Processing}
    \begin{itemize}
        \item \textbf{Real-Time Data Ingestion:} Developed and deployed \textbf{Apache Kafka}-based streaming pipelines to facilitate real-time data ingestion from core banking systems into the data lake.
        % \item \textbf{Scalable Architecture:} Designed a scalable stream processing architecture using \textbf{Kafka Streams} and \textbf{Flink} to handle high-throughput data streams with low latency.
        % \item \textbf{Monitoring and Alerting:} Implemented comprehensive monitoring solutions using \textbf{Prometheus} and \textbf{Grafana} to ensure the reliability and performance of streaming applications.
        \item \textbf{Data Transformation:} Created real-time data transformation jobs to cleanse and enrich incoming data streams, improving data quality for downstream analytics.
        \item \textbf{Cross-Functional Collaboration:} Worked closely with data scientists and analysts to ensure streaming data met analytical requirements and supported business objectives.
       \item \textbf{Kafka Connect Integration:} Implemented \textbf{Kafka Connect} connectors to streamline data ingestion from heterogeneous sources, reducing manual ETL code by \textbf{40\%} and improving connector maintainability.
    \end{itemize}
    \item \textbf{Automation:}
    \begin{itemize}
        \item \textbf{Informatica DEI Automation:} Developed Python automation suites to streamline Informatica DEI troubleshooting and environment object comparisons, reducing manual validation time by \textbf{40\%}.
        \item \textbf{Schema Reconciliation Tool:} Engineered an automated reconciliation tool between \textbf{ERwin data models} and physical schemas, achieving a \textbf{50\%} reduction in alignment efforts.
    \end{itemize}
        \item \textbf{SIT and UAT Support:}
    \begin{itemize}
        \item \textbf{Testing Leadership:} Led System Integration Testing (SIT) and User Acceptance Testing (UAT) efforts for new data pipelines and BI reports, ensuring alignment with business requirements.
        \item \textbf{Issue Resolution:} Collaborated with cross-functional teams to identify and resolve defects, achieving a \textbf{95\%} issue resolution rate prior to production deployment.
        \item \textbf{Stakeholder Collaboration:} Worked closely with business analysts and data consumers to validate data accuracy and report functionality during UAT phases.
        \item \textbf{Continuous Improvement:} Gathered feedback from testing phases to inform iterative improvements in data pipeline design and reporting features.
        \item \textbf{Cross-Functional Coordination:} Acted as a liaison between development, QA, and business teams to streamline communication and enhance testing efficiency.
        \item \textbf{Performance Monitoring:} Implemented monitoring tools to track the performance of data pipelines during testing, enabling proactive issue identification and resolution.
        \item \textbf{Post-Deployment Support:} Provided ongoing support post-deployment to address any issues arising from SIT and UAT, ensuring sustained data quality and system performance.
    \end{itemize}
    % \item \textbf{Data Engineering Operations \& Automation:}
    % \begin{itemize}
    %     \item \textbf{Real-Time Streaming:} Implemented stream-processing pipelines using \textbf{Kafka/Redpanda} for real-time data ingestion across core banking applications.
    %     \item \textbf{Python Automation Suites:} Developed custom tooling to automate Informatica DEI troubleshooting and environment object comparisons, reducing manual validation time by \textbf{40\%}.
    %     \item \textbf{Schema Integrity:} Engineered an automated reconciliation tool between \textbf{ERwin data models} and physical schemas, achieving a \textbf{50\%} reduction in alignment efforts.
    %     \item \textbf{Agile Leadership:} Drive delivery within \textbf{Agile squads} using JIRA/Confluence to translate complex business metrics into scalable dimensional models (Star/Snowflake).
    % \end{itemize}
\end{itemize}